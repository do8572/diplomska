@article{bay.opt,
  author    = {Tinu Theckel Joy and
               Santu Rana and
               Sunil Gupta and
               Svetha Venkatesh},
  title     = {Fast Hyperparameter Tuning using Bayesian Optimization with Directional
               Derivatives},
  journal   = {CoRR},
  volume    = {abs/1902.02416},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.02416},
  archivePrefix = {arXiv},
  eprint    = {1902.02416},
  timestamp = {Fri, 04 Oct 2019 14:45:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-02416.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bay.gen,
author = {Wu, J. and Chen, X.-Y and Zhang, H. and Xiong, L.-D and Lei, H. and Deng, S.-H},
year = {2019},
month = {03},
pages = {26-40},
title = {Hyperparameter optimization for machine learning models based on Bayesian optimization},
volume = {17},
journal = {Journal of Electronic Science and Technology},
doi = {10.11989/JEST.1674-862X.80904120}
}

@Inbook{Feurer2019,
author="Feurer, Matthias
and Hutter, Frank",
editor="Hutter, Frank
and Kotthoff, Lars
and Vanschoren, Joaquin",
title="Hyperparameter Optimization",
bookTitle="Automated Machine Learning: Methods, Systems, Challenges",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="3--33",
abstract="Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.",
isbn="978-3-030-05318-5",
doi="10.1007/978-3-030-05318-5_1",
url="https://doi.org/10.1007/978-3-030-05318-5_1"
}

@ARTICLE{HumanLoop,
  author={B. {Shahriari} and K. {Swersky} and Z. {Wang} and R. P. {Adams} and N. {de Freitas}},
  journal={Proceedings of the IEEE}, 
  title={Taking the Human Out of the Loop: A Review of Bayesian Optimization}, 
  year={2016},
  volume={104},
  number={1},
  pages={148-175},}

@article{DBLP:journals/corr/abs-1012-2599,
  author    = {Eric Brochu and
               Vlad M. Cora and
               Nando de Freitas},
  title     = {A Tutorial on Bayesian Optimization of Expensive Cost Functions, with
               Application to Active User Modeling and Hierarchical Reinforcement
               Learning},
  journal   = {CoRR},
  volume    = {abs/1012.2599},
  year      = {2010},
  url       = {http://arxiv.org/abs/1012.2599},
  archivePrefix = {arXiv},
  eprint    = {1012.2599},
  timestamp = {Mon, 13 Aug 2018 16:46:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1012-2599.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Kushner_1964,
	doi = {10.1115/1.3653121},
	url = {https://doi.org/10.1115%2F1.3653121},
	year = 1964,
	month = {mar},
	publisher = {{ASME} International},
	volume = {86},
	number = {1},
	pages = {97--106},
	author = {H. J. Kushner},
	title = {A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise},
	journal = {Journal of Basic Engineering}
}

@article{DBLP:journals/corr/abs-0912-3995,
  author    = {Niranjan Srinivas and
               Andreas Krause and
               Sham M. Kakade and
               Matthias W. Seeger},
  title     = {Gaussian Process Bandits without Regret: An Experimental Design Approach},
  journal   = {CoRR},
  volume    = {abs/0912.3995},
  year      = {2009},
  url       = {http://arxiv.org/abs/0912.3995},
  archivePrefix = {arXiv},
  eprint    = {0912.3995},
  timestamp = {Mon, 13 Aug 2018 16:45:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-0912-3995.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Misc{bayesian-optimization,
    author = {Fernando Nogueira},
    title = {{Bayesian Optimization}: Open source constrained global optimization tool for {Python}},
    year = {2014--},
    howpublished = {\url{https://github.com/fmfn/BayesianOptimization}},
}

@Misc{gpyopt,
author = {The GPyOpt authors},
title = {{GPyOpt}: A Bayesian Optimization framework in python},
howpublished = {\url{http://github.com/SheffieldML/GPyOpt}},
year = {2016}
}

@article{balandat2019botorch,
  Author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
  Journal = {arxiv e-prints},
  Title = {{BoTorch: Programmable Bayesian Optimization in PyTorch}},
  Year = {2019},
  url = {http://arxiv.org/abs/1910.06403}
}

@inproceedings{Lizotte2007AutomaticGO,
  title={Automatic Gait Optimization with Gaussian Process Regression},
  author={Daniel J. Lizotte and Tao Wang and Michael Bowling and Dale Schuurmans},
  booktitle={IJCAI},
  year={2007}
}

@article{arnold2019bayesian,
  title={Bayesian Optimization of Hyperparameters Using Gaussian Processes},
  author={Arnold, Jakub},
  year={2019},
  journal={Univerzita Karlova, Matematicko-fyzik{\'a}ln{\'\i} fakulta}
}